import os
import json
import re
import time
from dotenv import load_dotenv
from openrouter import OpenRouter
from supabase import create_client, Client
from tqdm import tqdm

load_dotenv()

# --- CONFIGURATION ---
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# Exact Model ID
EMBEDDING_MODEL = "qwen/qwen3-embedding-8b"
TARGET_DIMENSIONS = 1536 

# Performance settings
BATCH_SIZE = 100  # Much larger batch size for embeddings
DB_BATCH_SIZE = 500  # Larger DB insert batches

# --- CLIENTS ---
client = OpenRouter(api_key=OPENROUTER_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def clean_text(raw_text):
    """Removes Sefaria HTML tags/markers."""
    if not isinstance(raw_text, str): 
        return ""
    text = re.sub(r'<i data-commentator[^>]+></i>', '', raw_text)
    text = re.sub(r'<[^>]+>', '', text)
    return " ".join(text.split())

def get_embeddings_batch(texts):
    """Generates embeddings using the official OpenRouter SDK with server-side truncation."""
    try:
        response = client.embeddings.generate(
            model=EMBEDDING_MODEL,
            input=texts,
            dimensions=TARGET_DIMENSIONS 
        )
        return [item.embedding for item in response.data]
    except Exception as e:
        print(f"‚ö†Ô∏è Embedding Error: {e}")
        time.sleep(2)
        return None

def process_single_book(book_title, segments_dir):
    """Process a single book efficiently."""
    print(f"üìñ Processing: {book_title}...")
    
    # Get all segment files and sort them properly
    segment_files = []
    for file_name in os.listdir(segments_dir):
        if file_name.endswith('.json'):
            segment_files.append(os.path.join(segments_dir, file_name))
    
    # Sort files to maintain order (siman_1_seif_1, siman_1_seif_2, etc.)
    segment_files.sort()
    
    print(f"Found {len(segment_files)} segment files")
    
    # Process all files and collect data
    all_data = []
    
    print("üìã Reading segment files...")
    for segment_file in tqdm(segment_files, desc="Reading files"):
        try:
            with open(segment_file, 'r', encoding='utf-8') as f:
                segment_data = json.load(f)
            
            raw_text = segment_data.get('text', '')
            metadata = segment_data.get('metadata', {})
            
            cleaned_text = clean_text(raw_text)
            if not cleaned_text.strip(): 
                continue
            
            siman = metadata.get('siman', 1)
            seif = metadata.get('seif', 1)
            ref = f"{book_title} {siman}:{seif}"
            
            all_data.append({
                "content": cleaned_text,
                "ref": ref,
                "siman": siman,
                "seif": seif,
                "section": metadata.get("section", "Halakhah"),
                "book": book_title
            })
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing {segment_file}: {e}")
            continue
    
    # Sort by siman and seif to maintain order
    all_data.sort(key=lambda x: (x['siman'], x['seif']))
    
    print(f"‚úÖ Processed {len(all_data)} valid segments")
    
    # Generate embeddings in large batches
    all_texts = [item['content'] for item in all_data]
    all_embeddings = []
    
    print("üîÑ Generating embeddings in large batches...")
    for i in tqdm(range(0, len(all_texts), BATCH_SIZE), desc="Embeddings"):
        batch_texts = all_texts[i:i+BATCH_SIZE]
        
        # Retry logic for embeddings
        max_retries = 3
        for retry in range(max_retries):
            embeddings = get_embeddings_batch(batch_texts)
            if embeddings:
                all_embeddings.extend(embeddings)
                break
            elif retry < max_retries - 1:
                print(f"‚ö†Ô∏è Retry {retry + 1} for batch {i//BATCH_SIZE + 1}")
                time.sleep(2)
            else:
                print(f"‚ùå Failed to generate embeddings for batch {i//BATCH_SIZE + 1}")
                # Add dummy embeddings to maintain alignment
                all_embeddings.extend([[0.0]*TARGET_DIMENSIONS] * len(batch_texts))
    
    # Insert into database in large batches
    # IMPORTANT: Insert in order! BIGSERIAL (chunk_index) auto-increments in insertion order
    print("üíæ Inserting into database...")
    batch_records = []
    
    for i, data in enumerate(tqdm(all_data, desc="Database insert")):
        if i < len(all_embeddings):
            vector = all_embeddings[i]
            if len(vector) > TARGET_DIMENSIONS:
                vector = vector[:TARGET_DIMENSIONS]
            
            # chunk_index is auto-generated by BIGSERIAL - no need to specify it
            record = {
                "content": data['content'],
                "embedding": vector,
                "sefaria_ref": data['ref'],
                "metadata": {
                    "book": data['book'],
                    "siman": data['siman'],
                    "seif": data['seif'],
                    "section": data['section']
                }
            }
            
            batch_records.append(record)
            
            # Insert batch when full
            if len(batch_records) >= DB_BATCH_SIZE:
                try:
                    supabase.table("sefaria_text_chunks").insert(batch_records).execute()
                    print(f"‚úÖ Inserted {len(batch_records)} records")
                    batch_records = []
                except Exception as e:
                    print(f"‚ùå DB Error: {e}")
                    # Try smaller batches on error
                    for j in range(0, len(batch_records), 100):
                        small_batch = batch_records[j:j+100]
                        try:
                            supabase.table("sefaria_text_chunks").insert(small_batch).execute()
                            print(f"‚úÖ Inserted small batch of {len(small_batch)} records")
                        except Exception as e2:
                            print(f"‚ùå Small batch error: {e2}")
                    batch_records = []
    
    # Insert remaining records
    if batch_records:
        try:
            supabase.table("sefaria_text_chunks").insert(batch_records).execute()
            print(f"‚úÖ Final insert: {len(batch_records)} records")
        except Exception as e:
            print(f"‚ùå Final DB Error: {e}")
    
    print(f"‚úÖ Completed {book_title}")

if __name__ == "__main__":
    # All Shulchan Arukh books to process
    BOOKS = [
        {
            "title": "Shulchan Arukh, Choshen Mishpat",
            "segments_dir": "Shulchan-Arukh-Cleaned-With-Metadata/cleaned_with_metadata/Choshen Mishpat/segments"
        }
    ]
    
    for book in BOOKS:
        if os.path.exists(book['segments_dir']):
            process_single_book(book['title'], book['segments_dir'])
        else:
            print(f"‚ö†Ô∏è Missing directory: {book['segments_dir']}")
